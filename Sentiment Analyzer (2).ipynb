{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweet):\n",
    "    stemmer=PorterStemmer()\n",
    "    stop_words=stopwords.words('english')\n",
    "#     next 2 lines removes noise\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet)\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=True, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    clean_tweets=[]\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            clean_tweets.append(stemmed_word)\n",
    "    return clean_tweets\n",
    "            \n",
    "            \n",
    "def build_frequency(tweets, ys):\n",
    "    yslist=np.squeeze(ys).tolist()\n",
    "    frequency = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweets(tweet):\n",
    "            pair=(word, y)\n",
    "            if pair in frequency:\n",
    "                frequency[pair] += 1\n",
    "            else:\n",
    "                frequency[pair] =1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split tweets for training and testing.\n",
    "positive_tests=positive_tweets[4000:]\n",
    "positive_train=positive_tweets[:4000]\n",
    "negative_tests=negative_tweets[4000:]\n",
    "negative_train=negative_tweets[:4000]\n",
    "\n",
    "train_x=positive_train + negative_train\n",
    "test_x= positive_tests + negative_tests\n",
    "train_y = np.append(np.ones((len(positive_train), 1)), np.zeros((len(negative_train), 1))\n",
    ", axis=0)\n",
    "test_y = np.append(np.ones((len(positive_tests),1)), np.zeros((len(negative_tests), 1)), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = build_frequency(train_x, train_y)\n",
    "print(f'{len(frequency.keys())}')\n",
    "\n",
    "print(train_x[1])\n",
    "print(process_tweets(train_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    h=1/(1+np.exp(-z))\n",
    "    return h\n",
    "\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    m = x.shape[0]\n",
    "    for i in range(0, num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        h= sigmoid(z)\n",
    "        j= -1./m*(np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(), np.log(1-h)))\n",
    "        \n",
    "        theta = theta - (alpha/m)*np.dot(x.transpose(), (h-y))\n",
    "        j = float(j)\n",
    "        return j, theta\n",
    "    \n",
    "def extract_features(token, frequency):\n",
    "    word_token = process_tweets(token)\n",
    "    x = np.zeros((1,3))\n",
    "    x[0,0]=1\n",
    "\n",
    "    for word in word_token:\n",
    "        x[0,1]+=frequency.get((word, 1.0),0)\n",
    "        x[0,2] +=frequency.get((word, 0.0),0)\n",
    "        assert(x.shape ==(1,3))\n",
    "        return x\n",
    "\n",
    "X = np.zeros((len(train_x),3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]=extract_features(train_x[i], frequency)\n",
    "    \n",
    "Y = train_y\n",
    "J, theta = gradientDescent(X,Y, np.zeros((3,1)), 1e-9, 1500)\n",
    "    \n",
    "\n",
    "def predict_tweet(tweet, frequency, theta):\n",
    "    x = extract_features(tweet, frequency)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, frequency, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tweet test\n",
    "\n",
    "custom_tweet = 'best car'\n",
    "tweet_prediction=predict_tweet(custom_tweet, frequency, theta)\n",
    "if tweet_prediction > 0.5:\n",
    "    print('Positive')\n",
    "else:\n",
    "    print('Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data_filepath=\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\"\n",
    "test_data_filepath =  \"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_data['Content'])\n",
    "test_vectors = vectorizer.transform(test_data['Content'])\n",
    "\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_data['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# print(f'training time: {time_linear_train}, prediction time: {time_linear_predict} ')\n",
    "report  = classification_report(test_data['Label'], prediction_linear, output_dict=True)\n",
    "print(report['pos'])\n",
    "print(report['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets  = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets  = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data_filepath=\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\"\n",
    "test_data_filepath =  \"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_data['Content'])\n",
    "test_vectors = vectorizer.transform(test_data['Content'])\n",
    "\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_data['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# print(f'training time: {time_linear_train}, prediction time: {time_linear_predict} ')\n",
    "report  = classification_report(test_data['Label'], prediction_linear, output_dict=True)\n",
    "print(report['pos'])\n",
    "print(report['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data_filepath=\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\"\n",
    "test_data_filepath =  \"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_data['Content'])\n",
    "test_vectors = vectorizer.transform(test_data['Content'])\n",
    "\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_data['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# print(f'training time: {time_linear_train}, prediction time: {time_linear_predict} ')\n",
    "report  = classification_report(test_data['Label'], prediction_linear, output_dict=True)\n",
    "print(report['pos'])\n",
    "print(report['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets  = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets  = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')\n",
    "def remove_noise(tweet_tokens, stop_words=()):\n",
    "    cleaned_tweets =[]\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        if tag.startswith('NN'):\n",
    "            pos='n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos='v'\n",
    "        else:\n",
    "            pos='a'\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token,pos)\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tweets.append(token.lower())\n",
    "    return cleaned_tweets\n",
    "    \n",
    "    \n",
    "print(remove_noise(tweet_tokens[2], stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_uncleaned_tweets[13])\n",
    "print(positive_cleaned_tweets_list[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         convert tokens into dictionary for both positiev and negatice tweets\n",
    "\n",
    "def model_for_tweets(cleaned_tokens_list):\n",
    "    for tweets in cleaned_tokens_list:\n",
    "        yield dict([tweet, True] for tweet in tweets )\n",
    "        \n",
    "positive_tweets_model=model_for_tweets(positive_uncleaned_tweets)\n",
    "negative_tweets_model=model_for_tweets(negative_uncleaned_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split the dataset for training and testing the models\n",
    "\n",
    "import random\n",
    "positive_dataset=[(tweet_dict, 'Positive') for tweet_dict in positive_tweets_model]\n",
    "negative_dataset=[(tweet_dict, 'Negative') for tweet_dict in negative_tweets_model]\n",
    "\n",
    "dataset=positive_dataset+negative_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data=dataset[:7000]\n",
    "test_data=dataset[7000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is : \", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet=\"\"\"\n",
    "During opening statements in Derek Chauvin's trial in the death of George Floyd, prosecutors showed jurors a bystander video of the former officer kneeling on Floyd's neck.\n",
    "\"\"\"\n",
    "custom_tokens = remove_noise(nltk.word_tokenize(custom_tweet))\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data_filepath=\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\"\n",
    "test_data_filepath =  \"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_data['Content'])\n",
    "test_vectors = vectorizer.transform(test_data['Content'])\n",
    "\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_data['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# print(f'training time: {time_linear_train}, prediction time: {time_linear_predict} ')\n",
    "report  = classification_report(test_data['Label'], prediction_linear, output_dict=True)\n",
    "print(report['pos'])\n",
    "print(report['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  add custom tweet here \n",
    "custom_tweet=\"\"\"\n",
    "    #add custom tweet here\n",
    "\n",
    "\"\"\"\n",
    "custom_vector = vectorizer.transform([custom_text])\n",
    "print(classifier_linear.predict(custom_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  add custom tweet here \n",
    "custom_tweet=\"\"\"\n",
    "    #add custom tweet here\n",
    "\n",
    "\"\"\"\n",
    "custom_vector = vectorizer.transform([custom_text])\n",
    "print(classifier_linear.predict(custom_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets  = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets  = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data_filepath=\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\"\n",
    "test_data_filepath =  \"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_data['Content'])\n",
    "test_vectors = vectorizer.transform(test_data['Content'])\n",
    "\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_data['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# print(f'training time: {time_linear_train}, prediction time: {time_linear_predict} ')\n",
    "report  = classification_report(test_data['Label'], prediction_linear, output_dict=True)\n",
    "print(report['pos'])\n",
    "print(report['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  add custom tweet here \n",
    "custom_tweet=\"\"\"\n",
    "    #add custom tweet here\n",
    "\n",
    "\"\"\"\n",
    "custom_vector = vectorizer.transform([custom_text])\n",
    "print(classifier_linear.predict(custom_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\HP-PC/nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\HP-PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\hp-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples.zip/twitter_samples/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\HP-PC/nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\HP-PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5d0ef488f34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpositive_tweets\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtwitter_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'positive_tweets.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnegative_tweets\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtwitter_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'negative_tweets.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwitter_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweets.20150430-223406.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\HP-PC/nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\hp-pc\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\HP-PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "positive_tweets  = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets  = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "# # tweet_tokens[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@DespiteOfficial', 'we', 'have', 'a', 'listen', 'last', 'night', ':)', 'As', 'You', 'Bleed', 'be', 'an', 'amazing', 'track', '.', 'When', 'be', 'you', 'in', 'Scotland', '?', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(pos_tag(tweet_tokens[1]))\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos ='n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "print(lemmatize_sentence(tweet_tokens[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'last', 'night', ':)', 'bleed', 'amazing', 'track', 'scotland']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words=stopwords.words('english')\n",
    "def remove_noise(tweet_tokens, stop_words=()):\n",
    "    cleaned_tweets =[]\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        if tag.startswith('NN'):\n",
    "            pos='n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos='v'\n",
    "        else:\n",
    "            pos='a'\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token,pos)\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tweets.append(token.lower())\n",
    "    return cleaned_tweets\n",
    "    \n",
    "    \n",
    "print(remove_noise(tweet_tokens[2], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "positive_uncleaned_tweets=twitter_samples.tokenized('positive_tweets.json')\n",
    "positive_cleaned_tweets_list=[]\n",
    "negative_uncleaned_tweets=twitter_samples.tokenized('negative_tweets.json')\n",
    "negative_cleaned_tweets_list=[]\n",
    "\n",
    "for words in positive_uncleaned_tweets:\n",
    "    positive_cleaned_tweets_list.append(remove_noise(words, stop_words ))\n",
    "    \n",
    "for words in negative_uncleaned_tweets:\n",
    "    negative_cleaned_tweets_list.append(remove_noise(words, stop_words ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@jjulieredburn', 'Perfect', ',', 'so', 'you', 'already', 'know', \"what's\", 'waiting', 'for', 'you', ':)']\n",
      "['perfect', 'already', 'know', \"what's\", 'wait', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(positive_uncleaned_tweets[13])\n",
    "print(positive_cleaned_tweets_list[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: Prepare data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         convert tokens into dictionary for both positiev and negatice tweets\n",
    "\n",
    "def model_for_tweets(cleaned_tokens_list):\n",
    "    for tweets in cleaned_tokens_list:\n",
    "        yield dict([tweet, True] for tweet in tweets )\n",
    "        \n",
    "positive_tweets_model=model_for_tweets(positive_uncleaned_tweets)\n",
    "negative_tweets_model=model_for_tweets(negative_uncleaned_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split the dataset for training and testing the models\n",
    "\n",
    "import random\n",
    "positive_dataset=[(tweet_dict, 'Positive') for tweet_dict in positive_tweets_model]\n",
    "negative_dataset=[(tweet_dict, 'Negative') for tweet_dict in negative_tweets_model]\n",
    "\n",
    "dataset=positive_dataset+negative_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data=dataset[:7000]\n",
    "test_data=dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: Building and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2065.4 : 1.0\n",
      "                      :) = True           Positi : Negati =    986.0 : 1.0\n",
      "                     sad = True           Negati : Positi =     49.8 : 1.0\n",
      "                   Thank = True           Positi : Negati =     26.3 : 1.0\n",
      "                  THANKS = True           Negati : Positi =     24.4 : 1.0\n",
      "                 welcome = True           Positi : Negati =     22.9 : 1.0\n",
      "                  FOLLOW = True           Negati : Positi =     19.1 : 1.0\n",
      "                    THAT = True           Negati : Positi =     19.1 : 1.0\n",
      "                    miss = True           Negati : Positi =     18.5 : 1.0\n",
      "                    MUCH = True           Negati : Positi =     18.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is : \", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-18feec722bdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mDuring\u001b[0m \u001b[0mopening\u001b[0m \u001b[0mstatements\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDerek\u001b[0m \u001b[0mChauvin\u001b[0m\u001b[1;34m's trial in the death of George Floyd, prosecutors showed jurors a bystander video of the former officer kneeling on Floyd'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mneck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcustom_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcustom_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_noise' is not defined"
     ]
    }
   ],
   "source": [
    "custom_tweet=\"\"\"\n",
    "During opening statements in Derek Chauvin's trial in the death of George Floyd, prosecutors showed jurors a bystander video of the former officer kneeling on Floyd's neck.\n",
    "\"\"\"\n",
    "custom_tokens = remove_noise(nltk.word_tokenize(custom_tweet))\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
